{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.28510219, -0.21361618, -0.00468576, -1.86013234,  1.84238025,\n",
       "         -0.40951539, -0.53836045, -0.52704631],\n",
       "        [-0.89695075, -1.00895405, -0.83749199,  0.93796229, -1.21645196,\n",
       "          0.61239255, -0.75620668, -0.18775102],\n",
       "        [ 0.86570504, -1.02519589, -0.32764229, -0.62259984, -1.56114   ,\n",
       "         -2.11567798, -0.00892709,  1.25706832],\n",
       "        [-0.28797953, -0.41004683,  2.6873104 , -1.50403716, -0.26002903,\n",
       "          1.22397   , -0.65568646,  2.24927254]]),\n",
       " array([[-5.18476288e-01,  2.38313015e-01,  3.36328785e-01,\n",
       "          1.94731855e-02, -1.80993620e-01,  1.14522335e+00,\n",
       "         -5.00267683e-01,  1.24965011e+00],\n",
       "        [ 5.27354756e-01, -2.11144691e-01,  9.84897854e-01,\n",
       "         -1.05407432e+00,  1.26387909e+00,  1.77559954e+00,\n",
       "          5.64745381e-01,  1.41759127e-03],\n",
       "        [-8.43719732e-01,  2.15499802e+00, -1.34713987e+00,\n",
       "          1.80658846e-01, -1.48284572e+00, -4.73379789e-01,\n",
       "          1.61858326e-01,  3.93551771e-01],\n",
       "        [-2.79691239e-02,  9.54441214e-01,  1.69463428e+00,\n",
       "         -4.37507769e-01,  1.14677320e+00, -8.75178076e-01,\n",
       "          6.87536285e-01,  4.34911065e-01]]),\n",
       " array([[-0.35554833, -0.81257688, -0.50922826, -0.09947729,  1.68311892,\n",
       "          0.84877992,  1.89642857,  2.11791562],\n",
       "        [ 0.60233557, -0.69023998, -1.21728777, -1.0257043 , -0.13388554,\n",
       "         -0.86421889, -0.16933033, -1.96382984],\n",
       "        [ 1.36886733,  0.01181896, -2.08348936,  0.85781643, -0.67495647,\n",
       "          0.642517  ,  1.16771881,  0.23954914],\n",
       "        [-0.22824649, -1.48696825,  0.7814041 , -0.68186242, -0.49390641,\n",
       "          0.79865077,  1.00268558, -0.06553394]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "q, k, v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.4282697 ,  3.4481835 , -3.86329326,  2.46585413],\n",
       "        [ 1.02637224, -2.95091664,  1.19777933, -5.30003038],\n",
       "        [-1.38049054, -4.72637792,  1.19891977, -0.68364712],\n",
       "        [ 5.51372756,  5.644333  , -3.94731954,  3.98676444]]),\n",
       " array([[-1.4282697 ,  3.4481835 , -3.86329326,  2.46585413],\n",
       "        [ 1.02637224, -2.95091664,  1.19777933, -5.30003038],\n",
       "        [-1.38049054, -4.72637792,  1.19891977, -0.68364712],\n",
       "        [ 5.51372756,  5.644333  , -3.94731954,  3.98676444]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T), np.einsum(\"ij,kj->ik\", q, k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why we need sqrt(d_k) in denominator\n",
    "To reduce it's variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2591072378909316, 0.8003583179557943, 12.131480935404243)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.einsum(\"ij,kj->ik\", q, k).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2591072378909316, 0.8003583179557943, 1.5164351169255301)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = (np.einsum(\"ij,kj->ik\", q, k)/ np.sqrt(d_k))\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "- This is to ensure words don't get context from words generated in future\n",
    "- Not required in encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L, L)))\n",
    "mask\n",
    "\n",
    "## Consider the sentence -> My name is Arun\n",
    "## Now see this matrix as for each word, i.,e in first row my can only see my\n",
    "## similarly name can see \"my\" and \"name\", ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty \n",
    "mask[mask == 1] = 0\n",
    "mask\n",
    "\n",
    "## see below why do we do this\n",
    "## -inf because we are going to apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.5049696 ,  1.21911697, -1.36588043,  0.87181109],\n",
       "        [ 0.36287738, -1.04330658,  0.42347894, -1.87384371],\n",
       "        [-0.48807711, -1.67102694,  0.42388215, -0.24170576],\n",
       "        [ 1.94939707,  1.99557307, -1.39558821,  1.40953409]]),\n",
       " array([[-0.5049696 ,        -inf,        -inf,        -inf],\n",
       "        [ 0.36287738, -1.04330658,        -inf,        -inf],\n",
       "        [-0.48807711, -1.67102694,  0.42388215,        -inf],\n",
       "        [ 1.94939707,  1.99557307, -1.39558821,  1.40953409]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled, scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.80316336, 0.19683664, 0.        , 0.        ],\n",
       "       [0.26346515, 0.08071878, 0.65581607, 0.        ],\n",
       "       [0.37518559, 0.39291638, 0.01322932, 0.21866871]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T\n",
    "\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 8))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v, v\n",
    "attention.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q [[ 0.28510219 -0.21361618 -0.00468576 -1.86013234  1.84238025 -0.40951539\n",
      "  -0.53836045 -0.52704631]\n",
      " [-0.89695075 -1.00895405 -0.83749199  0.93796229 -1.21645196  0.61239255\n",
      "  -0.75620668 -0.18775102]\n",
      " [ 0.86570504 -1.02519589 -0.32764229 -0.62259984 -1.56114    -2.11567798\n",
      "  -0.00892709  1.25706832]\n",
      " [-0.28797953 -0.41004683  2.6873104  -1.50403716 -0.26002903  1.22397\n",
      "  -0.65568646  2.24927254]]\n",
      "K [[-5.18476288e-01  2.38313015e-01  3.36328785e-01  1.94731855e-02\n",
      "  -1.80993620e-01  1.14522335e+00 -5.00267683e-01  1.24965011e+00]\n",
      " [ 5.27354756e-01 -2.11144691e-01  9.84897854e-01 -1.05407432e+00\n",
      "   1.26387909e+00  1.77559954e+00  5.64745381e-01  1.41759127e-03]\n",
      " [-8.43719732e-01  2.15499802e+00 -1.34713987e+00  1.80658846e-01\n",
      "  -1.48284572e+00 -4.73379789e-01  1.61858326e-01  3.93551771e-01]\n",
      " [-2.79691239e-02  9.54441214e-01  1.69463428e+00 -4.37507769e-01\n",
      "   1.14677320e+00 -8.75178076e-01  6.87536285e-01  4.34911065e-01]]\n",
      "V [[-0.35554833 -0.81257688 -0.50922826 -0.09947729  1.68311892  0.84877992\n",
      "   1.89642857  2.11791562]\n",
      " [ 0.60233557 -0.69023998 -1.21728777 -1.0257043  -0.13388554 -0.86421889\n",
      "  -0.16933033 -1.96382984]\n",
      " [ 1.36886733  0.01181896 -2.08348936  0.85781643 -0.67495647  0.642517\n",
      "   1.16771881  0.23954914]\n",
      " [-0.22824649 -1.48696825  0.7814041  -0.68186242 -0.49390641  0.79865077\n",
      "   1.00268558 -0.06553394]]\n",
      "New V [[ 0.24529629 -0.96154448 -0.46576911 -0.74506414 -0.11916575 -0.05105776\n",
      "   0.49246995 -0.82352518]\n",
      " [ 0.50617113 -0.46720515 -1.21678324  0.20201083  0.36466336  0.58191022\n",
      "   1.32651998  0.78038979]\n",
      " [ 0.58017519 -0.57084738 -0.99876499  0.16732969 -0.13199941  0.6315597\n",
      "   1.18898679  0.39974674]\n",
      " [ 0.07146972 -0.9010708  -0.52604193 -0.57809212  0.46194506  0.16202424\n",
      "   0.8796841   0.01182937]]\n",
      "Attention [[0.09097278 0.51012045 0.03846118 0.36044559]\n",
      " [0.41418838 0.10150793 0.440065   0.04423869]\n",
      " [0.19704655 0.06036987 0.49048723 0.25209635]\n",
      " [0.37518559 0.39291638 0.01322932 0.21866871]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention\n",
    "\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "print(f\"Q {q}\")\n",
    "print(f\"K {k}\")\n",
    "print(f\"V {v}\")\n",
    "print(f\"New V {values}\")\n",
    "print(f\"Attention {attention}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
