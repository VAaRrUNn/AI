{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047621f3f5cb4bebb894fb6ab15a544d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717c4c96b1d9460484d0c5dd1c3599ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\material\\Machine_Deep\\Deep Learning\\code\\Specialization\\Gan's\\mine\\Pix2Pix\\main.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m    Make a function so that use can use it as a wrapper function\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    to calculate time :).\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m losses \u001b[39m=\u001b[39m train(n_epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m       dataloader,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m       gen_opt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m       disc_opt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m       loss_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m       losses,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m       gen,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m       disc,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m       pixel_dis_lambda,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m       device,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/material/Machine_Deep/Deep%20Learning/code/Specialization/Gan%27s/mine/Pix2Pix/main.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m       )\n",
      "File \u001b[1;32md:\\material\\Machine_Deep\\Deep Learning\\code\\Specialization\\Gan's\\mine\\Pix2Pix\\train.py:30\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(n_epochs, dataloader, gen_opt, disc_opt, loss_fn, losses, gen, disc, pixel_dis_lambda, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m# Discriminator\u001b[39;00m\n\u001b[0;32m     29\u001b[0m disc_opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 30\u001b[0m fake_images \u001b[39m=\u001b[39m gen(s_images)\n\u001b[0;32m     31\u001b[0m disc_fake_pred \u001b[39m=\u001b[39m disc(fake_images)\n\u001b[0;32m     32\u001b[0m disc_real_pred \u001b[39m=\u001b[39m disc(r_images)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\material\\Machine_Deep\\Deep Learning\\code\\Specialization\\Gan's\\mine\\Pix2Pix\\model.py:133\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, noise)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, noise):\n\u001b[1;32m--> 133\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(noise)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\material\\Machine_Deep\\Deep Learning\\code\\Specialization\\Gan's\\mine\\Pix2Pix\\model.py:111\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m block \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample[i]\n\u001b[0;32m    108\u001b[0m out \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mConvTranspose2d(conv_in, conv_out,\n\u001b[0;32m    109\u001b[0m                          kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    110\u001b[0m                          stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)(out)\n\u001b[1;32m--> 111\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(\n\u001b[0;32m    112\u001b[0m     (out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mskip_connection_layers[skip_len \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m i]), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    113\u001b[0m \u001b[39m# print(out.shape)\u001b[39;00m\n\u001b[0;32m    114\u001b[0m out \u001b[39m=\u001b[39m block(out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from dataset import *\n",
    "from train import *\n",
    "from time import time\n",
    "\n",
    "\n",
    "def calculate_time():\n",
    "    \"\"\"\n",
    "    Make a function so that use can use it as a wrapper function\n",
    "    to calculate time :).\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "losses = train(n_epochs,\n",
    "      dataloader,\n",
    "      gen_opt,\n",
    "      disc_opt,\n",
    "      loss_fn,\n",
    "      losses,\n",
    "      gen,\n",
    "      disc,\n",
    "      pixel_dis_lambda,\n",
    "      device,\n",
    "      )\n",
    "\n",
    "# visualization\n",
    "\n",
    "# saving model weights\n",
    "# torch.save(obj=gen.state_dict(), f=\"pix2pix_1_gen.pth\")\n",
    "# torch.save(obj=disc.state_dict(), f=\"pix2pix_1_disc.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
