{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled after permute -> torch.Size([8, 2, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_heads,\n",
    "                 n_embd,\n",
    "                 mask = None):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.mask = mask\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.qkv_layer = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.linear_layer = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self,\n",
    "                input_embeddings):\n",
    "        \n",
    "        batch_size, max_seq_len, n_embd = input_embeddings.size()\n",
    "        qkv = self.qkv_layer(input_embeddings)\n",
    "        qkv = qkv.reshape((batch_size, max_seq_len ,self.n_heads, 3 * self.head_dim))\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) // n_embd\n",
    "        print(f\"scaled after permute -> {attention.permute(1, 0, 2, 3).shape}\")\n",
    "        if self.mask:\n",
    "            # print(f\"Attention shape -> {attention.shape}\")\n",
    "            # print(f\"scaled after permute -> {attention.permute(1, 0, 2, 3).shape}\")\n",
    "            attention += self.mask\n",
    "            # print(f\"scaled after permute -> {attention.permute(1, 0, 2, 3).shape}\")\n",
    "        # print(f\"Attention shape -> {attention.shape}\")\n",
    "\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        # print(f\"attention shape -> {attention.shape}\")\n",
    "        updated_emb = torch.matmul(attention, v)\n",
    "        # print(f\"updated emb -> {updated_emb.shape}\")\n",
    "        \n",
    "        updated_emb = updated_emb.permute(0, 2, 1, 3).reshape(batch_size, max_seq_len, n_embd)\n",
    "        return self.linear_layer(updated_emb)\n",
    "\n",
    "ma = MultiHeadAttention(n_heads=8, n_embd=512)\n",
    "x = torch.randn(2, 4, 512)\n",
    "ma(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "    def forward(self,\n",
    "                input_encodings):\n",
    "        \n",
    "        batch_size, max_seq_len, n_embd = input_encodings.size()\n",
    "        i = torch.arange(0, n_embd, 2).float()\n",
    "\n",
    "        denominator = torch.pow(10_000, (i)/n_embd).type(torch.float32)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float32).reshape(max_seq_len, 1)\n",
    "        even_PE = torch.sin(position/ denominator)\n",
    "        odd_PE = torch.cos(position/ denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2).view(max_seq_len, -1)\n",
    "        return stacked.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "    \n",
    "p = PositionalEncoding()\n",
    "p(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossMultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_heads,\n",
    "                 n_embd, \n",
    "                 mask = None):\n",
    "        \n",
    "        super(CrossMultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.mask = mask\n",
    "        self.head_dim = n_embd // n_heads\n",
    "        self.kv_layer = nn.Linear(n_embd, 2 * n_embd)\n",
    "        self.q_layer = nn.Linear(n_embd, n_embd)\n",
    "        self.linear_layer = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self,\n",
    "                kv,\n",
    "                q):\n",
    "        \n",
    "        batch_size, max_seq_len, n_embd = kv.size()\n",
    "        kv = self.kv_layer(kv)\n",
    "        kv = kv.reshape((batch_size, max_seq_len ,self.n_heads, 2 * self.head_dim))\n",
    "        q = self.q_layer(q)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.reshape(batch_size, max_seq_len, self.n_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        attention = torch.matmul(q, k.transpose(-2, -1)) // n_embd\n",
    "        if self.mask:\n",
    "            attention += self.mask\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        updated_emb = torch.matmul(attention, v)\n",
    "        \n",
    "        updated_emb = updated_emb.permute(0, 2, 1, 3).reshape(batch_size, max_seq_len, n_embd)\n",
    "        return self.linear_layer(updated_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_heads,\n",
    "                 n_embd, \n",
    "                 mask = None):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.mask = mask \n",
    "        self.positional_encodings = PositionalEncoding()\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.multiheadattention = MultiHeadAttention(n_heads=self.n_heads,\n",
    "                                                     n_embd=self.n_embd,\n",
    "                                                     mask=mask)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                input_embeddings):\n",
    "        out = self.positional_encodings(input_embeddings)\n",
    "        attention = self.multiheadattention(input_embeddings=input_embeddings)\n",
    "        attention = self.dropout(attention)\n",
    "        attention = self.layer_norm(attention + out)\n",
    "        # print(f\"attention shape -> {attention.shape}\")\n",
    "        updated_embeddings = self.feedforward(attention)\n",
    "        updated_embeddings = self.layer_norm(updated_embeddings + attention)\n",
    "        return updated_embeddings\n",
    "    \n",
    "model = EncoderLayer(n_heads=8,\n",
    "                n_embd=512)\n",
    "model(x).shape\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_heads,\n",
    "                 n_embd,\n",
    "                 mask = None):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_embd = n_embd\n",
    "        self.mask = mask\n",
    "        self.positional_encodings = PositionalEncoding()\n",
    "        self.layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.crossmultiheadattention = CrossMultiHeadAttention(n_heads=n_heads,\n",
    "                                                               n_embd=n_embd,\n",
    "                                                               mask=mask)\n",
    "        self.multiheadattention = MultiHeadAttention(n_heads=n_heads,\n",
    "                                                     n_embd=n_embd,\n",
    "                                                     mask=mask)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * n_embd),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2 * n_embd, n_embd),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                output_embeddings,\n",
    "                input_embeddings):\n",
    "        \n",
    "        output_embeddings = self.positional_encodings(output_embeddings)\n",
    "        attention = self.multiheadattention(output_embeddings)\n",
    "\n",
    "        # print(f\"Attention size -> {attention.shape}\")\n",
    "        q = self.layer_norm(attention + output_embeddings)\n",
    "        kv = input_embeddings\n",
    "\n",
    "        updated_attention = self.crossmultiheadattention(kv=kv,\n",
    "                                                         q=q)\n",
    "        updated_attention = self.layer_norm(updated_attention + q)\n",
    "        final_weights = self.feedforward(updated_attention)\n",
    "        final_weights = self.layer_norm(updated_attention + final_weights)\n",
    "\n",
    "        return final_weights\n",
    "    \n",
    "decoder = DecoderLayer(n_heads=8,\n",
    "                       n_embd=512)\n",
    "out = decoder(x, x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 n_heads,\n",
    "                 n_embd,\n",
    "                 mask=None):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(EncoderLayer(n_heads=n_heads,\n",
    "                                            n_embd=n_embd,\n",
    "                                            mask=mask))\n",
    "            \n",
    "\n",
    "    def forward(self,\n",
    "               input_embeddings,):\n",
    "        \n",
    "        out = input_embeddings\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "enc = Encoder(2, 8, 512)\n",
    "enc(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 n_heads, \n",
    "                 n_embd,\n",
    "                 mask=None):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(\n",
    "                DecoderLayer(n_heads=n_heads,\n",
    "                             n_embd=n_embd,\n",
    "                             mask=mask)\n",
    "            )\n",
    "\n",
    "        self.Liner = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, \n",
    "                output_embeddings,\n",
    "                input_embeddings):\n",
    "        \n",
    "        in_e = input_embeddings\n",
    "        out_e = output_embeddings\n",
    "        for layer in self.layers:\n",
    "            out_e = layer(input_embeddings=in_e,\n",
    "                        output_embeddings=out_e)\n",
    "            \n",
    "        return out_e\n",
    "    \n",
    "deco = Decoder(num_layers=2,\n",
    "               n_heads=8,\n",
    "               n_embd=512)\n",
    "\n",
    "            \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_heads,\n",
    "                 n_embd,\n",
    "                 encoder_mask=None,\n",
    "                 decoder_mask=None,\n",
    "                 num_encoder_layers=3,\n",
    "                 num_decoder_layers=3,\n",
    "                 vocab_size_input=70,\n",
    "                 vocab_size_output=70):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        # It changes (Batch_size, Max_seq_len, vocab_size) -> (Batch_size, max_seq_len, n_embd)\n",
    "        # For encoder part\n",
    "        self.input_embeddings = nn.Linear(vocab_size_input, n_embd)\n",
    "\n",
    "        # For decoder part\n",
    "        self.output_embeddings = nn.Linear(vocab_size_output, n_embd)\n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_encoder_layers,\n",
    "                               n_heads=n_heads,\n",
    "                               n_embd=n_embd,\n",
    "                               mask=encoder_mask)\n",
    "        self.deocder = Decoder(num_layers=num_decoder_layers,\n",
    "                               n_embd=n_embd,\n",
    "                               n_heads=n_heads,\n",
    "                               mask=decoder_mask)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(n_embd, 2 * vocab_size_output),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * vocab_size_output, vocab_size_output), \n",
    "            nn.ReLU(), \n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self,\n",
    "                input_sentence_embeddings,\n",
    "                output_sentence_embeddings):\n",
    "        \n",
    "        input_sentence_embeddings = self.input_embeddings(input_sentence_embeddings)\n",
    "        output_sentence_embeddings = self.output_embeddings(output_sentence_embeddings)\n",
    "\n",
    "        enc_out = self.encoder(input_sentence_embeddings)\n",
    "        dec_out = self.deocder(output_embeddings=output_sentence_embeddings,\n",
    "                               input_embeddings=input_sentence_embeddings)\n",
    "        out = self.final_layer(dec_out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = Encoder(n_heads=8,\n",
    "                  n_embd=512,\n",
    "                  num_layers=3)\n",
    "decoder = Decoder(n_heads=8,\n",
    "                  n_embd=512,\n",
    "                  num_layers=2)\n",
    "enc_out = encoder(x)\n",
    "dec_out = decoder(enc_out, x)\n",
    "enc_out.shape, dec_out.shape\n",
    "\n",
    "\n",
    "x_new = torch.randn(4, 300, 70).float()\n",
    "transformer = Transformer(n_heads=8,\n",
    "                          n_embd=512)\n",
    "output = transformer(x_new,\n",
    "            x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 135)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = 0x090\n",
    "ee = 0x005A \n",
    "combinations = list(\"0123456789ABCDEF\")\n",
    "\n",
    "hindi_vocab = []\n",
    "for codepoint in range(0x900, 0x980):\n",
    "    character = chr(codepoint)\n",
    "    hindi_vocab.append(character)\n",
    "\n",
    "\n",
    "\n",
    "START_TOKEN = '+'\n",
    "PADDING_TOKEN = '-'\n",
    "END_TOKEN = '_'\n",
    "\n",
    "\n",
    "english_vocab_start = 0x0041\n",
    "english_vocab_end = 0x005A\n",
    "\n",
    "english_vocab = [chr(code) for code in range(english_vocab_start, english_vocab_end + 1) \n",
    "                   if 'L' in unicodedata.category(chr(code))]\n",
    "\n",
    "\n",
    "## For lower case alphabets\n",
    "english_vocab_start = 0x0061\n",
    "english_vocab_end = 0x007A\n",
    "\n",
    "english_vocab2 = [chr(code) for code in range(english_vocab_start, english_vocab_end + 1) \n",
    "                   if 'L' in unicodedata.category(chr(code))]\n",
    "\n",
    "english_vocab += english_vocab2\n",
    "\n",
    "hindi_vocab.insert(0, START_TOKEN)\n",
    "english_vocab.insert(0, START_TOKEN)\n",
    "\n",
    "english_vocab.extend(list(\",.!`:;\"))\n",
    "english_vocab.extend(list(\"0123456789@#$%^&*()\"))\n",
    "english_vocab.append(' ')\n",
    "english_vocab.append(\"'\")\n",
    "\n",
    "hindi_vocab.append(\"'\")\n",
    "hindi_vocab.append(\" \")\n",
    "hindi_vocab.append(',')\n",
    "hindi_vocab.append('.')\n",
    "\n",
    "hindi_vocab.insert(0, PADDING_TOKEN)\n",
    "english_vocab.insert(0, PADDING_TOKEN)\n",
    "\n",
    "hindi_vocab.insert(0, END_TOKEN)\n",
    "english_vocab.insert(0, END_TOKEN)\n",
    "\n",
    "# Print the Hindi alphabets\n",
    "len(english_vocab), len(hindi_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135 135\n",
      "82 82\n"
     ]
    }
   ],
   "source": [
    "index_to_hindi = {k:v for k, v in enumerate(hindi_vocab)}\n",
    "index_to_english = {k:v for k, v in enumerate(english_vocab)}\n",
    "\n",
    "hindi_to_index = {v:k for k, v in enumerate(hindi_vocab)}\n",
    "english_to_index = {v:k for k, v in enumerate(english_vocab)}\n",
    "\n",
    "print(len(index_to_hindi), len(hindi_to_index))\n",
    "print(len(index_to_english), len(english_to_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got ' at 1\n",
      "I'd like to tell you about one such child,\",\" मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "with open(\"dataset/Hindi_English_Truncated_Corpus.csv\", 'r') as f:\n",
    "    l = f.readlines()\n",
    "\n",
    "full_sentense = l[2].split(',', 1)[1].strip('\\n\"')\n",
    "full_sentense\n",
    "for i in range(len(full_sentense)):\n",
    "    if full_sentense[i] in hindi_vocab:\n",
    "        print(f\"We got {full_sentense[i]} at {i}\")\n",
    "        break\n",
    "print(full_sentense[:45], full_sentense[45:])\n",
    "\n",
    "dataset_path = \"dataset/Hindi_English_Truncated_Corpus.csv\"\n",
    "\n",
    "def get_first_index(sentence, vocab, vocab2):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in vocab and sentence[i] != \" \" and sentence[i] not in vocab2:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def get_dataset():\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    english_sentences, hindi_sentences = [], []\n",
    "    # Skipping first line cause it's header\n",
    "    # i = 0\n",
    "    for a in range(1, len(lines)):\n",
    "        # print(f\"lines -> {lines[a]}\")\n",
    "        line = lines[a].split(',', 1)[1].strip('\\n\"')\n",
    "        # print(f\"after -> {line}\")\n",
    "        index = get_first_index(line, hindi_vocab, english_vocab)\n",
    "        if index == -1:\n",
    "            continue\n",
    "        eng = line[:index].strip('\",?')\n",
    "        hin = line[index:].strip('\",.!_`?')\n",
    "        # print(f\"eng -> {eng}\")\n",
    "        # print(f\"hin -> {hin}\\n\")\n",
    "        english_sentences.append(eng)\n",
    "        hindi_sentences.append(hin)\n",
    "        # if i== 3:\n",
    "        #     break\n",
    "        # i+= 1\n",
    "    \n",
    "    return english_sentences, hindi_sentences\n",
    "\n",
    "english_sentences, hindi_sentences = get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['politicians do not have permission to do what needs to be done.',\n",
       "  \"I'd like to tell you about one such child\",\n",
       "  'This percentage is even greater than the percentage in India.',\n",
       "  \"what we really mean is that they're bad at not paying attention.\"],\n",
       " ['राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है ',\n",
       "  'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी',\n",
       "  'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।',\n",
       "  'हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते',\n",
       "  'इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:4], hindi_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97th percentile length Kannada: 267.0\n",
      "97th percentile length English: 265.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "PERCENTILE = 97\n",
    "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in hindi_sentences], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Hindi: 127575\n",
      "Number of sentences in English: 127575\n",
      "Number of valid sentences: 90289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "def is_valid_token(sentence, vocab):\n",
    "    # for token in list(set(sentence)):\n",
    "    for token in sentence:\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "    \n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1)\n",
    "\n",
    "valid_sentence_indices = []\n",
    "for index in range(len(hindi_sentences)):\n",
    "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(hindi_sentence, MAX_SEQUENCE_LENGTH) \\\n",
    "        and is_valid_token(hindi_sentence, hindi_vocab) \\\n",
    "        and is_valid_token(english_sentence, english_vocab) \\\n",
    "        and is_valid_length(english_sentence, MAX_SEQUENCE_LENGTH):\n",
    "        valid_sentence_indices.append(index)\n",
    "\n",
    "print(f\"Number of sentences in Hindi: {len(hindi_sentences)}\")\n",
    "print(f\"Number of sentences in English: {len(english_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indices)}\")\n",
    "valid_sentence_indices[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90289"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_valid_token(sentence, vocab):\n",
    "    # for token in list(set(sentence)):\n",
    "    for token in sentence:\n",
    "        if token not in vocab:\n",
    "            print(f\"is this space{token}yeah\")\n",
    "            print(\"huh\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "is_valid_token(hindi_sentences[0], hindi_vocab), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['politicians do not have permission to do what needs to be done.',\n",
       "  \"I'd like to tell you about one such child\",\n",
       "  'This percentage is even greater than the percentage in India.',\n",
       "  \"what we really mean is that they're bad at not paying attention.\",\n",
       "  '.The ending portion of these Vedas is called Upanishad.',\n",
       "  'The then Governor of Kashmir resisted transfer , but was finally reduced to subjection with the aid of British .',\n",
       "  'In this lies the circumstances of people before you.',\n",
       "  'And who are we to say, even, that they are wrong',\n",
       "  '“”Global Warming“” refer to warming caused in recent decades and probability of its continual presence and its indirect effect on human being.',\n",
       "  \"You may want your child to go to a school that is not run by the LEA - a non-maintained special school or an independent school that can meet your child 's needs .\"],\n",
       " ['राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है ',\n",
       "  'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी',\n",
       "  'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।',\n",
       "  'हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते',\n",
       "  'इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।',\n",
       "  'कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का विरोध किया था , लेकिन अंग्रेजों की सहायता से उनकी आवाज दबा दी गयी ',\n",
       "  'इसमें तुमसे पूर्व गुज़रे हुए लोगों के हालात हैं।',\n",
       "  'और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं',\n",
       "  'ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों में हुई वार्मिंग और इसके निरंतर बने रहने के अनुमान और इसके अप्रत्यक्ष रूप से मानव पर पड़ने वाले प्रभाव से है।',\n",
       "  'हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटेन्ड ह्यबिना किसी समर्थन के हृ विशेष स्कूल , या किसी स्वतंत्र स्कूल में जाए , इजसके पास विशेष शैक्षणिक जऋऋरतों वाले बच्चों के प्रति सहूलियत हों . '])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:10], hindi_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = [hindi_sentences[idx] for idx in valid_sentence_indices]\n",
    "english_sentences = [english_sentences[idx] for idx in valid_sentence_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('People feel that forests are their open treasure-houses for them to use as they feel like .',\n",
       "  'We will send you details about claiming expenses when we write to tell you about the arrangements for the hearing .',\n",
       "  'Stopping smoking',\n",
       "  \"Microsoft's induct I.M.I\"),\n",
       " ('लोग यह मानते हैं कि वन उनके लिए खुले खजाने की तरह हैं जिनका वे जैसा चाहें वैसा उपयोग कर सकते हैं ',\n",
       "  'ख़र्चे क्लेम करने के विवरण हम आप को तब भेजेंगे जब हम आप को सुनवाई के प्रबन्धों के बारे में लिखेंगे ',\n",
       "  'धूम्रपान बंद करें',\n",
       "  'माइक्रोसाफ्ट का इण्डिक आईएमई'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, english_sentences, hindi_sentences):\n",
    "        self.english_sentences = english_sentences \n",
    "        self.hindi_sentences = hindi_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.hindi_sentences[idx]\n",
    "dataset = TextDataset(english_sentences, hindi_sentences)\n",
    "len(dataset), dataset[100]\n",
    "\n",
    "train_dataloader = DataLoader(dataset=dataset,\n",
    "                              batch_size=32,\n",
    "                              shuffle=True)\n",
    "a, b = next(iter(train_dataloader))\n",
    "a[:4], b[:4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=hindi_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(shape: tuple):\n",
    "  mask = torch.full(shape, float('-inf'))\n",
    "  mask = torch.triu(mask, diagonal=1)\n",
    "  return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mask = create_masks((1, 8, 4, 4))\n",
    "new_mask[0][0] # There are 8 of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 300, 135])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_to_tensor(char,\n",
    "                   vocab_to_index,\n",
    "                   vocab_size):\n",
    "    \n",
    "    char_tensor = torch.zeros((vocab_size))\n",
    "    char_tensor[vocab_to_index[char]] = 1\n",
    "    return char_tensor\n",
    "\n",
    "\n",
    "def sentence_to_tensor(sen,\n",
    "                       vocab_to_index,\n",
    "                       vocab_size,\n",
    "                       max_seq_len):\n",
    "    padding_tensor = char_to_tensor(char=PADDING_TOKEN,\n",
    "                                    vocab_to_index=vocab_to_index,\n",
    "                                    vocab_size=vocab_size)\n",
    "    sen_tensor = torch.stack([padding_tensor] * max_seq_len)\n",
    "    for i, char in enumerate(sen):\n",
    "        char_tensor = char_to_tensor(char=char,\n",
    "                       vocab_to_index=vocab_to_index,\n",
    "                       vocab_size=vocab_size)\n",
    "        sen_tensor[i] = char_tensor\n",
    "    return sen_tensor\n",
    "\n",
    "\n",
    "def sentences_to_tensor(sentences,\n",
    "                        vocab_to_index,\n",
    "                        vocab_size,\n",
    "                        max_seq_len):\n",
    "    batch_size = len(sentences)\n",
    "    sens_tensor = torch.zeros(batch_size, max_seq_len, vocab_size)\n",
    "    for i, sen in enumerate(sentences):\n",
    "        sen_tensor = sentence_to_tensor(sen=sen,\n",
    "                                        vocab_to_index=vocab_to_index,\n",
    "                                        vocab_size=vocab_size,\n",
    "                                        max_seq_len=max_seq_len)\n",
    "        sens_tensor[i] = sen_tensor\n",
    "    return sens_tensor\n",
    "\n",
    "sen_tensor = sentence_to_tensor(english_sentences[0], english_to_index, len(english_vocab), 300)\n",
    "sentences_to_tensor(hindi_sentences[:5], hindi_to_index, len(hindi_vocab), 300).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Using inbuilt transformer network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InbuiltTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 inp_mask=None,\n",
    "                 out_mask=None):\n",
    "        \n",
    "        super(InbuiltTransformer, self).__init__()\n",
    "        self.inp_mask = inp_mask\n",
    "        self.out_mask = out_mask\n",
    "\n",
    "        self.input_embeddings = nn.Linear(input_vocab_size, d_model)\n",
    "        self.output_embeddings = nn.Linear(output_vocab_size, d_model)\n",
    "\n",
    "        self.trans = nn.Transformer(d_model=d_model,\n",
    "                                    nhead=n_heads,\n",
    "                                    num_encoder_layers=num_encoder_layers,\n",
    "                                    num_decoder_layers=num_decoder_layers,\n",
    "                                    dim_feedforward=1024,\n",
    "                                    batch_first=True)\n",
    "        \n",
    "    def forward(self,\n",
    "                input_embeddings,\n",
    "                output_embeddings):\n",
    "        \n",
    "        input_embeddings = self.input_embeddings(input_embeddings)\n",
    "        output_embeddings = self.output_embeddings(output_embeddings)\n",
    "        out = self.trans(src=input_embeddings,\n",
    "                   tgt=output_embeddings,\n",
    "                   tgt_mask=self.out_mask,\n",
    "                   src_mask=self.inp_mask)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 8\n",
    "N_EMBD = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "mask = create_masks((MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH))\n",
    "transformer = InbuiltTransformer(d_model=N_EMBD,\n",
    "                                 n_heads=N_HEADS,\n",
    "                                 num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                                 num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                                 input_vocab_size=len(english_vocab),\n",
    "                                 output_vocab_size=len(hindi_vocab),\n",
    "                                 out_mask=mask)\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=hindi_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m hin_sens \u001b[39m=\u001b[39m sentences_to_tensor(sentences\u001b[39m=\u001b[39mhin_sens,\n\u001b[0;32m      5\u001b[0m                                vocab_to_index\u001b[39m=\u001b[39mhindi_to_index,\n\u001b[0;32m      6\u001b[0m                                vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(hindi_vocab),\n\u001b[0;32m      7\u001b[0m                                max_seq_len\u001b[39m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[0;32m      9\u001b[0m eng_sens \u001b[39m=\u001b[39m sentences_to_tensor(sentences\u001b[39m=\u001b[39meng_sens,\n\u001b[0;32m     10\u001b[0m                                vocab_to_index\u001b[39m=\u001b[39menglish_to_index,\n\u001b[0;32m     11\u001b[0m                                vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(english_vocab),\n\u001b[0;32m     12\u001b[0m                                max_seq_len\u001b[39m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[1;32m---> 15\u001b[0m out \u001b[39m=\u001b[39m transformer(input_embeddings\u001b[39m=\u001b[39;49meng_sens,\n\u001b[0;32m     16\u001b[0m                   output_embeddings\u001b[39m=\u001b[39;49mhin_sens)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[84], line 36\u001b[0m, in \u001b[0;36mInbuiltTransformer.forward\u001b[1;34m(self, input_embeddings, output_embeddings)\u001b[0m\n\u001b[0;32m     34\u001b[0m input_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_embeddings(input_embeddings)\n\u001b[0;32m     35\u001b[0m output_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_embeddings(output_embeddings)\n\u001b[1;32m---> 36\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrans(src\u001b[39m=\u001b[39;49minput_embeddings,\n\u001b[0;32m     37\u001b[0m            tgt\u001b[39m=\u001b[39;49moutput_embeddings,\n\u001b[0;32m     38\u001b[0m            tgt_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_mask,\n\u001b[0;32m     39\u001b[0m            src_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minp_mask)\n\u001b[0;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:146\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(src, mask\u001b[39m=\u001b[39msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39msrc_key_padding_mask)\n\u001b[1;32m--> 146\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask, memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    148\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    149\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:360\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    357\u001b[0m output \u001b[39m=\u001b[39m tgt\n\u001b[0;32m    359\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> 360\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[0;32m    361\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[0;32m    362\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[0;32m    363\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:698\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    696\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x))\n\u001b[0;32m    697\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 698\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m    699\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[0;32m    700\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:707\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    706\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    708\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    709\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    710\u001b[0m                        is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[0;32m    711\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    712\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1189\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1175\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1176\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1177\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1187\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[0;32m   1188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1190\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1191\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1193\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1194\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1195\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m   1196\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1197\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m   1198\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[0;32m   1199\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[0;32m   1200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1201\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\functional.py:5334\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5331\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5332\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5334\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[0;32m   5335\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5337\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "for eng_sens, hin_sens in train_dataloader:\n",
    "\n",
    "    hin_sens = sentences_to_tensor(sentences=hin_sens,\n",
    "                                   vocab_to_index=hindi_to_index,\n",
    "                                   vocab_size=len(hindi_vocab),\n",
    "                                   max_seq_len=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    eng_sens = sentences_to_tensor(sentences=eng_sens,\n",
    "                                   vocab_to_index=english_to_index,\n",
    "                                   vocab_size=len(english_vocab),\n",
    "                                   max_seq_len=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    \n",
    "    out = transformer(input_embeddings=eng_sens,\n",
    "                      output_embeddings=hin_sens)\n",
    "    print(out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Using Custom made)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HEADS = 8\n",
    "N_EMBD = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "mask = create_masks((1, 1, N_HEADS, N_HEADS))\n",
    "transformer = Transformer(n_heads=N_HEADS,\n",
    "                          n_embd=N_EMBD,\n",
    "                          decoder_mask=None,\n",
    "                          num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "                          num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                          vocab_size_input=len(english_vocab),\n",
    "                          vocab_size_output=len(hindi_vocab))\n",
    "\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=hindi_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300, 135])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m hin_sens \u001b[39m=\u001b[39m sentences_to_tensor(sentences\u001b[39m=\u001b[39mhin_sens,\n\u001b[0;32m      5\u001b[0m                                vocab_to_index\u001b[39m=\u001b[39mhindi_to_index,\n\u001b[0;32m      6\u001b[0m                                vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(hindi_vocab),\n\u001b[0;32m      7\u001b[0m                                max_seq_len\u001b[39m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[0;32m      9\u001b[0m eng_sens \u001b[39m=\u001b[39m sentences_to_tensor(sentences\u001b[39m=\u001b[39meng_sens,\n\u001b[0;32m     10\u001b[0m                                vocab_to_index\u001b[39m=\u001b[39menglish_to_index,\n\u001b[0;32m     11\u001b[0m                                vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(english_vocab),\n\u001b[0;32m     12\u001b[0m                                max_seq_len\u001b[39m=\u001b[39mMAX_SEQUENCE_LENGTH)\n\u001b[1;32m---> 15\u001b[0m out \u001b[39m=\u001b[39m transformer(eng_sens,\n\u001b[0;32m     16\u001b[0m                   hin_sens)\n\u001b[0;32m     17\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 80\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, input_sentence_embeddings, output_sentence_embeddings)\u001b[0m\n\u001b[0;32m     77\u001b[0m output_sentence_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_embeddings(output_sentence_embeddings)\n\u001b[0;32m     79\u001b[0m enc_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(input_sentence_embeddings)\n\u001b[1;32m---> 80\u001b[0m dec_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeocder(output_embeddings\u001b[39m=\u001b[39;49moutput_sentence_embeddings,\n\u001b[0;32m     81\u001b[0m                        input_embeddings\u001b[39m=\u001b[39;49minput_sentence_embeddings)\n\u001b[0;32m     82\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer(dec_out)\n\u001b[0;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, output_embeddings, input_embeddings)\u001b[0m\n\u001b[0;32m     24\u001b[0m out_e \u001b[39m=\u001b[39m output_embeddings\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 26\u001b[0m     out_e \u001b[39m=\u001b[39m layer(input_embeddings\u001b[39m=\u001b[39;49min_e,\n\u001b[0;32m     27\u001b[0m                 output_embeddings\u001b[39m=\u001b[39;49mout_e)\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m out_e\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 41\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, output_embeddings, input_embeddings)\u001b[0m\n\u001b[0;32m     38\u001b[0m updated_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrossmultiheadattention(kv\u001b[39m=\u001b[39mkv,\n\u001b[0;32m     39\u001b[0m                                                  q\u001b[39m=\u001b[39mq)\n\u001b[0;32m     40\u001b[0m updated_attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(updated_attention \u001b[39m+\u001b[39m q)\n\u001b[1;32m---> 41\u001b[0m final_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeedforward(updated_attention)\n\u001b[0;32m     42\u001b[0m final_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(updated_attention \u001b[39m+\u001b[39m final_weights)\n\u001b[0;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m final_weights\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sanat\\.conda\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "for eng_sens, hin_sens in train_dataloader:\n",
    "\n",
    "    hin_sens = sentences_to_tensor(sentences=hin_sens,\n",
    "                                   vocab_to_index=hindi_to_index,\n",
    "                                   vocab_size=len(hindi_vocab),\n",
    "                                   max_seq_len=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    eng_sens = sentences_to_tensor(sentences=eng_sens,\n",
    "                                   vocab_to_index=english_to_index,\n",
    "                                   vocab_size=len(english_vocab),\n",
    "                                   max_seq_len=MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    \n",
    "    out = transformer(eng_sens,\n",
    "                      hin_sens)\n",
    "    print(out.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (3687628710.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[112], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    mask.shape, (a+mask, dim = -1)\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((8, 10, 4, 4))\n",
    "mask.shape, (torch.sum((mask, a), dim=01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
