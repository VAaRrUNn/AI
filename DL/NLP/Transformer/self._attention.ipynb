{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.68190211, -0.94305326,  0.07893161, -0.03624853,  0.45741098,\n",
       "         -0.44829929, -0.24813051,  1.45937929],\n",
       "        [-1.14309258, -0.57973174, -1.0412738 ,  0.94769575,  0.05873672,\n",
       "          0.19985851,  1.36647004, -0.13683946],\n",
       "        [-0.19955125, -0.42674957,  0.22373456, -1.32353963, -1.44472564,\n",
       "          1.46642934,  0.44756885,  0.12039613],\n",
       "        [ 0.1985702 ,  0.97842152, -0.03877199, -1.0715728 , -0.50543109,\n",
       "          0.75281765,  0.55503435, -0.97773298]]),\n",
       " array([[ 0.0760034 ,  1.21254768,  0.11962905,  0.40280899,  2.08565063,\n",
       "         -0.46898407, -0.52284828, -0.24912036],\n",
       "        [-1.07594132, -1.38524548,  1.55240629, -1.27517719,  0.07886014,\n",
       "          1.11728429, -0.49441666, -0.83698795],\n",
       "        [ 0.55275853,  0.43031192, -2.00058078,  0.94076683, -1.73413799,\n",
       "         -0.33796419, -1.03680266,  0.49883279],\n",
       "        [ 1.11621141, -0.09282793,  1.0912765 , -1.30005241, -1.01872229,\n",
       "          2.19376868, -0.81599013, -0.51761354]]),\n",
       " array([[ 1.23745848,  1.69623919, -0.68700635,  0.06021433, -0.39171069,\n",
       "         -1.04289411,  0.46420511, -0.97087047],\n",
       "        [-0.86684658, -0.99046743,  0.7318422 , -1.4798993 ,  0.94394208,\n",
       "          0.65871688,  0.70616477, -0.63943664],\n",
       "        [ 0.91787938, -0.66083491, -0.20710469,  0.23519263,  0.63024732,\n",
       "         -0.63413858, -0.79815278, -0.80049863],\n",
       "        [-0.13017792,  1.10659265,  0.52290837, -0.8493781 ,  0.37471494,\n",
       "          0.25412524, -0.31836762,  0.3311658 ]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)\n",
    "q, k, v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.27006439,  0.64519485, -0.63120076, -2.54270617],\n",
       "        [-1.18425073, -1.12513051,  0.43897104, -4.25607863],\n",
       "        [-5.00391795,  4.04336443, -0.38091035,  6.0429506 ],\n",
       "        [-0.68864038,  1.0824369 , -0.84087035,  3.70119829]]),\n",
       " array([[-0.27006439,  0.64519485, -0.63120076, -2.54270617],\n",
       "        [-1.18425073, -1.12513051,  0.43897104, -4.25607863],\n",
       "        [-5.00391795,  4.04336443, -0.38091035,  6.0429506 ],\n",
       "        [-0.68864038,  1.0824369 , -0.84087035,  3.70119829]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T), np.einsum(\"ij,kj->ik\", q, k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why we need sqrt(d_k) in denominator\n",
    "To reduce it's variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6284323494612694, 1.1413571687505781, 7.648227968818941)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var(), k.var(), np.einsum(\"ij,kj->ik\", q, k).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6284323494612694, 1.1413571687505781, 0.9560284961023675)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = (np.einsum(\"ij,kj->ik\", q, k)/ np.sqrt(d_k))\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "- This is to ensure words don't get context from words generated in future\n",
    "- Not required in encoders, but required in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L, L)))\n",
    "mask\n",
    "\n",
    "## Consider the sentence -> My name is Arun\n",
    "## Now see this matrix as for each word, i.,e in first row my can only see my\n",
    "## similarly name can see \"my\" and \"name\", ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty \n",
    "mask[mask == 1] = 0\n",
    "mask\n",
    "\n",
    "## see below why do we do this\n",
    "## -inf because we are going to apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.09548218,  0.22811083, -0.22316317, -0.89898239],\n",
       "        [-0.41869586, -0.39779371,  0.1551997 , -1.50475103],\n",
       "        [-1.76915216,  1.4295452 , -0.13467215,  2.13650567],\n",
       "        [-0.24347114,  0.38269924, -0.29729256,  1.3085712 ]]),\n",
       " array([[-0.09548218,        -inf,        -inf,        -inf],\n",
       "        [-0.41869586, -0.39779371,        -inf,        -inf],\n",
       "        [-1.76915216,  1.4295452 , -0.13467215,        -inf],\n",
       "        [-0.24347114,  0.38269924, -0.29729256,  1.3085712 ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled, scaled + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.49477465, 0.50522535, 0.        , 0.        ],\n",
       "       [0.03265052, 0.79995709, 0.1673924 , 0.        ],\n",
       "       [0.11710785, 0.21904247, 0.11097155, 0.55287814]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T\n",
    "\n",
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 4), (4, 8))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v, v\n",
    "attention.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q [[-0.68190211 -0.94305326  0.07893161 -0.03624853  0.45741098 -0.44829929\n",
      "  -0.24813051  1.45937929]\n",
      " [-1.14309258 -0.57973174 -1.0412738   0.94769575  0.05873672  0.19985851\n",
      "   1.36647004 -0.13683946]\n",
      " [-0.19955125 -0.42674957  0.22373456 -1.32353963 -1.44472564  1.46642934\n",
      "   0.44756885  0.12039613]\n",
      " [ 0.1985702   0.97842152 -0.03877199 -1.0715728  -0.50543109  0.75281765\n",
      "   0.55503435 -0.97773298]]\n",
      "K [[ 0.0760034   1.21254768  0.11962905  0.40280899  2.08565063 -0.46898407\n",
      "  -0.52284828 -0.24912036]\n",
      " [-1.07594132 -1.38524548  1.55240629 -1.27517719  0.07886014  1.11728429\n",
      "  -0.49441666 -0.83698795]\n",
      " [ 0.55275853  0.43031192 -2.00058078  0.94076683 -1.73413799 -0.33796419\n",
      "  -1.03680266  0.49883279]\n",
      " [ 1.11621141 -0.09282793  1.0912765  -1.30005241 -1.01872229  2.19376868\n",
      "  -0.81599013 -0.51761354]]\n",
      "V [[ 1.23745848  1.69623919 -0.68700635  0.06021433 -0.39171069 -1.04289411\n",
      "   0.46420511 -0.97087047]\n",
      " [-0.86684658 -0.99046743  0.7318422  -1.4798993   0.94394208  0.65871688\n",
      "   0.70616477 -0.63943664]\n",
      " [ 0.91787938 -0.66083491 -0.20710469  0.23519263  0.63024732 -0.63413858\n",
      "  -0.79815278 -0.80049863]\n",
      " [-0.13017792  1.10659265  0.52290837 -0.8493781   0.37471494  0.25412524\n",
      "  -0.31836762  0.3311658 ]]\n",
      "New V [[ 0.21266211  0.06501191  0.10143418 -0.58179525  0.44080624 -0.15548118\n",
      "   0.16041907 -0.6498394 ]\n",
      " [ 0.46875396 -0.02775081 -0.03165257 -0.31935     0.43965169 -0.34113299\n",
      "  -0.08201435 -0.70952254]\n",
      " [-0.27098222  0.36134671  0.52498113 -0.961172    0.55514096  0.30470133\n",
      "  -0.02669253 -0.05339561]\n",
      " [-0.01507415  0.52016549  0.34597257 -0.76061212  0.43800223  0.09228484\n",
      "  -0.05554861 -0.15949857]]\n",
      "Attention [[0.26954339 0.3725318  0.23723433 0.12069047]\n",
      " [0.24190604 0.24701561 0.42942407 0.08165428]\n",
      " [0.01245173 0.30507474 0.06383741 0.61863612]\n",
      " [0.11710785 0.21904247 0.11097155 0.55287814]]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis = -1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention\n",
    "\n",
    "values, attention = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "print(f\"Q {q}\")\n",
    "print(f\"K {k}\")\n",
    "print(f\"V {v}\")\n",
    "print(f\"New V {values}\")\n",
    "print(f\"Attention {attention}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
